:py:mod:`spann.model`
=====================

.. py:module:: spann.model


Module Contents
---------------

Classes
~~~~~~~

.. autoapisummary::

   spann.model.DSBatchNorm
   spann.model.Block
   spann.model.NN
   spann.model.Encoder
   spann.model.Decoder
   spann.model.ProtoCLS
   spann.model.CLS
   spann.model.CrossEntropyLabelSmooth
   spann.model.SPANN_model



Functions
~~~~~~~~~

.. autoapisummary::

   spann.model.kl_div
   spann.model.distance_matrix
   spann.model.distance_gmm
   spann.model.calculate_bimodality_coefficient
   spann.model.dip
   spann.model.gcm_cal
   spann.model.lcm_cal
   spann.model.sup_diff
   spann.model.p_table
   spann.model.ubot_CCD
   spann.model.ensemble_score
   spann.model.mixup_target
   spann.model.pdists



Attributes
~~~~~~~~~~

.. autoapisummary::

   spann.model.activation


.. py:function:: kl_div(mu, var, weight=None)


.. py:function:: distance_matrix(pts_src: torch.Tensor, pts_dst: torch.Tensor, p: int = 2)


.. py:function:: distance_gmm(mu_src: torch.Tensor, mu_dst: torch.Tensor, var_src: torch.Tensor, var_dst: torch.Tensor)


.. py:function:: calculate_bimodality_coefficient(series=None)

   calculate BC-coefficient, higher BC coef means more likely to be a bimodal distribution


.. py:function:: dip(samples, num_bins=100, p=0.99, table=True)

   dip test for distribution, to test is the distribution id multi-modality


.. py:function:: gcm_cal(cdf, idxs)


.. py:function:: lcm_cal(cdf, idxs)


.. py:function:: sup_diff(alpha, beta, contact_points)


.. py:function:: p_table(p, ans, sample_size, n_samples)


.. py:data:: activation

   

.. py:class:: DSBatchNorm(num_features, n_domain, eps=1e-05, momentum=0.1)


   Bases: :py:obj:`torch.nn.Module`

   Domain-specific Batch Normalization Layer
   :num_features: dimension of the features
   :n_domain: domain number


   .. py:method:: reset_running_stats()


   .. py:method:: reset_parameters()


   .. py:method:: _check_input_dim(input)
      :abstractmethod:


   .. py:method:: forward(x, y)



.. py:class:: Block(input_dim, output_dim, norm='', act='', dropout=0)


   Bases: :py:obj:`torch.nn.Module`

   Basic block consist of:
       fc -> bn -> act -> dropout
   :input_dim: dimension of input
   : output_dim: dimension of output
   :norm: batch normalization, '' represent no batch normalization, 1 represent regular batch normalization, int>1 represent domain-specific batch normalization of n domain
   :act: activation function, relu -> nn.ReLU, rrelu -> nn.RReLU, sigmoid -> nn.Sigmoid(), leaky_relu -> nn.LeakyReLU(), tanh -> nn.Tanh(),  '' -> None
   dropout: dropout rate

   .. py:method:: forward(x, y=None)



.. py:class:: NN(input_dim, cfg)


   Bases: :py:obj:`torch.nn.Module`

   Neural network consist of multi Blocks
   :input_dim: input dimension
   :cfg: model structure configuration, example - [['fc', x_dim, n_domain, 'sigmoid']]

   .. py:method:: forward(x, y=None)



.. py:class:: Encoder(input_dim, cfg)


   Bases: :py:obj:`torch.nn.Module`

   VAE Encoder
   :input_dim: input dimension
   :cfg: encoder configuration, example - [['fc', 1024, 1, 'relu'],['fc', 10, '', '']]

   .. py:method:: reparameterize(mu, var)


   .. py:method:: forward(x, domain, y=None)

      



.. py:class:: Decoder(z_dim, cfg)


   Bases: :py:obj:`torch.nn.Module`

   VAE Decoder
   :z_dim: latent dimension
   :cfg: decoder configuration, example - [['fc', adatas[i].obsm[obsm[i]].shape[1], 1, 'sigmoid']]


   .. py:method:: forward(z, domain, y=None)

      



.. py:class:: ProtoCLS(in_dim, out_dim, temp=0.05)


   Bases: :py:obj:`torch.nn.Module`

   prototype-based classifier
   L2-norm + a fc layer (without bias)

   .. py:method:: forward(x)


   .. py:method:: weight_norm()



.. py:class:: CLS(in_dim, out_dim, hidden_mlp=1024, feat_dim=16, temp=0.05)


   Bases: :py:obj:`torch.nn.Module`

   a classifier made up of projection head and prototype-based classifier

   .. py:method:: forward(x)



.. py:class:: CrossEntropyLabelSmooth(num_classes, device, epsilon=0.1, reduction=True)


   Bases: :py:obj:`torch.nn.Module`

   Cross entropy loss with label smoothing regularizer
   :num_classes (int): number of classes
   :epsilon (float): weight


   .. py:method:: forward(inputs, targets)

      :inputs: prediction matrix (before softmax) with shape (batch_size, num_classes)
      :targets: ground truth labels with shape (num_classes)



.. py:function:: ubot_CCD(sim, beta, device, stopThr=0.0001)

   Unbalanced optimal transport function
   :sim: similarity matrix between two distributions
   :beta: marginal distribution of the transport object
   :return: pseudo label computed from the tranport plan, updated marginal distribution of the transport object, normarlized transport plan, confidence score of the transport plan



.. py:function:: ensemble_score(logit_tensor, confidence_ot)

   



.. py:function:: mixup_target(x_batch, device)


.. py:function:: pdists(A, squared=False, eps=0)


.. py:class:: SPANN_model(x_dim, z_dim, enc, dec, class_num, device)


   Bases: :py:obj:`object`

   SPANN model, using scRNA reference dataset to annotate spatial transcriptome dataset

   :x_dim: input dimension for the encoder
   :z_dim: latent feature dimension
   :enc: encoder parameters, for example, [['fc', 1024, 1, 'relu'], ['fc', 16, '', '']]
   :dec: decoder parameters, dec is a set {cm_dec_params, spa_dec_params, rna_dec_params}, each element shapes like enc
   :class_num: number of scRNA-seq cell types
   :device: device on which SPAMM model is aranged


   .. py:method:: train(source_cm_dl, target_cm_dl, source_sp_ds, target_sp_ds, spatial_coor, test_source_cm_dl, test_target_cm_dl, source_labels, cell_types, lr=0.0001, lambda_recon=2000, lambda_kl=0.5, lambda_spa=0.5, lambda_cd=0.001, lambda_nb=0.1, mu=0.6, temp=0.1, k=20, resolution=0.5, novel_cell_test=True, maxiter=6000, miditer1=2000, miditer2=5000, miditer3=4000, test_freq=1000)

      Training and evaluating function for SPANN applying on test spatial dataset with ground truth labels

      :source_cm_dl: torch dataloader of the common genes scRNA-seq data
      :target_cm_dl: torch dataloader of the common genes spatial data
      :source_sp_ds: torch dataset of the scRNA-seq specific genes data
      :target_sp_ds: torch dataset of the spatial specific genes data
      :spatial_coor: pandas dataframe of the raw spatial coordinates, column names ['X','Y']
      :test_source_cm_dl: torch dataloader of the common genes scRNA-seq data for test
      :test_target_cm_dl: torch dataloader of the common genes spatial data for test
      :source_labels: integer labels for scRNA-seq data
      :cell_types: list of all cell types exist in scRNA-seq dataset

      :lr: learning rate for VAE and classifier, default=1e-4
      :lamnda_recon: training weight for the reconstruction loss, default=2000
      :lambda_kl: training weight for the KL-divergence loss, default=0.5
      :lambda_spa: training weight for the adjacency loss, default=0.5, we recomend to set it smaller when the spatial expression pattern is weak
      :lambda_cd: training weight for the unbalanced optimal transport alignment loss, default=0.001, we recomend to set it higher when the gap between scRNA-seq and spatial datasets is big
      :lambda_nb: training weight for the neighbor loss, default=0.1
      :mu: updating speed of beta by moving average, default=0.5
      :resolution: the expected minimum proportion of known cells, 0 means no constraints on novel cell discovery and 1 means no novel cells, default=0.5
      :novel_cell_test: whether to apply dip test and compute BC co-efficient to adaptively select resolution, default=True
      :maxiter: maximum iteration, default=6000
      :miditer1: after which iteration SPANN starts to conduct UOT alignment, default=2000
      :miditer2: after which iteration SPANN starts to train with neighbor loss, default=5000
      :miditer3: after which iteration SPANN starts to train with adjacency loss, default=4000
      :test_freq: test frequency, default=1000

      :return: two AnnData objects, adata_source and adata_target, containing the spatial coordinates, latent embeddings, E-scores, predictions and etc.



   .. py:method:: train_eval(source_cm_dl, target_cm_dl, source_sp_ds, target_sp_ds, spatial_coor, test_source_cm_dl, test_target_cm_dl, source_labels, target_labels, cell_types, common_cell_type, lr=0.0001, lambda_recon=2000, lambda_kl=0.5, lambda_spa=0.5, lambda_cd=0.001, lambda_nb=0.1, mu=0.6, temp=0.1, k=20, resolution=0.5, novel_cell_test=True, maxiter=6000, miditer1=2000, miditer2=5000, miditer3=4000, test_freq=1000)

      Training and evaluating function for SPANN applying on test spatial dataset with ground truth labels

      :source_cm_dl: torch dataloader of the common genes scRNA-seq data
      :target_cm_dl: torch dataloader of the common genes spatial data
      :source_sp_ds: torch dataset of the scRNA-seq specific genes data
      :target_sp_ds: torch dataset of the spatial specific genes data
      :spatial_coor: pandas dataframe of the raw spatial coordinates, column names ['X','Y']
      :test_source_cm_dl: torch dataloader of the common genes scRNA-seq data for test or validate
      :test_target_cm_dl: torch dataloader of the common genes spatial data for test or validate
      :source_labels: integer labels for scRNA-seq data
      :target_labels: integer labels for spatial data
      :cell_types: list of all cell types exist in scRNA-seq or spatial datasets
      :common_cell_types: list of cell types exist in both scRNA-seq and spatial datasets

      :lr: learning rate for VAE and classifier, default=1e-4
      :lamnda_recon: training weight for the reconstruction loss, default=2000
      :lambda_kl: training weight for the KL-divergence loss, default=0.5
      :lambda_spa: training weight for the adjacency loss, default=0.5, we recomend to set it smaller when the spatial expression pattern is weak
      :lambda_cd: training weight for the unbalanced optimal transport alignment loss, default=0.001, we recomend to set it higher when the gap between scRNA-seq and spatial datasets is big
      :lambda_nb: training weight for the neighbor loss, default=0.1
      :mu: updating speed of beta by moving average, default=0.5
      :resolution: the expected minimum proportion of known cells, 0 means no constraints on novel cell discovery and 1 means no novel cells, default=0.5
      :novel_cell_test: whether to apply dip test and compute BC co-efficient to adaptively select resolution, default=True
      :maxiter: maximum iteration, default=6000
      :miditer1: after which iteration SPANN starts to conduct UOT alignment, default=2000
      :miditer2: after which iteration SPANN starts to train with neighbor loss, default=5000
      :miditer3: after which iteration SPANN starts to train with adjacency loss, default=4000
      :test_freq: test frequency, default=1000

      :return: two AnnData objects, adata_source and adata_target, containing the spatial coordinates, latent embeddings, E-scores, predictions and etc.




